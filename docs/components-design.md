# SubGen ç»„ä»¶åŒ–è®¾è®¡æ–‡æ¡£ â€” æŒ‰éœ€ä¸‹è½½æ¶æ„

> ç›®æ ‡ï¼šexe æœ¬ä½“ä¿æŒè½»é‡ï¼ˆ~50MBï¼‰ï¼Œç”¨æˆ·é€‰æ‹©éœ€è¦çš„åŠŸèƒ½åæŒ‰éœ€ä¸‹è½½å¯¹åº”ç»„ä»¶ã€‚
> ç±»ä¼¼ Ollamaã€VS Code Extensions çš„æ¨¡å¼ã€‚

---

## 1. æ ¸å¿ƒç†å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  subgen.exe (~50MB)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CLI + Engine + ç¿»è¯‘SDK + æ ·å¼/é¡¹ç›®ç³»ç»Ÿ       â”‚  â”‚
â”‚  â”‚  äº‘ç«¯ Whisper (Groq/OpenAI API)               â”‚  â”‚
â”‚  â”‚  OAuth (Copilot/ChatGPT)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
           subgen init / é¦–æ¬¡è¿è¡Œ
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ~/.subgen/                                         â”‚
â”‚  â”œâ”€â”€ config.yaml          â† ç”¨æˆ·é…ç½®               â”‚
â”‚  â”œâ”€â”€ auth/                â† OAuth tokens            â”‚
â”‚  â”œâ”€â”€ bin/                 â† æŒ‰éœ€ä¸‹è½½çš„äºŒè¿›åˆ¶        â”‚
â”‚  â”‚   â”œâ”€â”€ ffmpeg(.exe)     â† å¯é€‰, ~80MB            â”‚
â”‚  â”‚   â””â”€â”€ whisper-cpp(.exe)â† æœ¬åœ° Whisper å¼•æ“      â”‚
â”‚  â”‚       â”œâ”€â”€ cuda/        â† CUDA ç‰ˆ, ~15MB         â”‚
â”‚  â”‚       â””â”€â”€ cpu/         â† CPU ç‰ˆ, ~5MB           â”‚
â”‚  â””â”€â”€ models/              â† æŒ‰éœ€ä¸‹è½½çš„æ¨¡å‹          â”‚
â”‚      â””â”€â”€ whisper/                                   â”‚
â”‚          â”œâ”€â”€ ggml-tiny.bin       â† 75MB             â”‚
â”‚          â”œâ”€â”€ ggml-base.bin       â† 142MB            â”‚
â”‚          â”œâ”€â”€ ggml-small.bin      â† 466MB            â”‚
â”‚          â”œâ”€â”€ ggml-medium.bin     â† 1.5GB            â”‚
â”‚          â””â”€â”€ ggml-large-v3.bin   â† 3.1GB            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**åŸåˆ™ï¼š**
- exe åªåŒ…å«èƒ½ç”¨ Python çº¯åŒ…å®ç°çš„åŠŸèƒ½
- ä»»ä½•éœ€è¦ç¼–è¯‘/å¤§ä½“ç§¯çš„ç»„ä»¶éƒ½èµ°æŒ‰éœ€ä¸‹è½½
- ä¸‹è½½æœ‰è¿›åº¦æ¡ã€æœ‰æ ¡éªŒã€å¯æ–­ç‚¹ç»­ä¼ 
- å·²ä¸‹è½½çš„ç»„ä»¶æœ‰ç‰ˆæœ¬ç®¡ç†ï¼Œå¯æ›´æ–°

---

## 2. ç»„ä»¶æ¸…å•

### 2.1 å†…ç½®ç»„ä»¶ï¼ˆéš exe æ‰“åŒ…ï¼‰

| ç»„ä»¶ | ç”¨é€” | ä¾èµ– |
|------|------|------|
| CLI ç•Œé¢ | å‘½ä»¤è¡Œäº¤äº’ | click, rich |
| SubGenEngine | æ ¸å¿ƒå¼•æ“ | â€” |
| ç¿»è¯‘æ¨¡å— | LLM ç¿»è¯‘/æ ¡å¯¹ | openai, anthropic, httpx |
| å­—å¹•æ¸²æŸ“ | SRT/ASS/VTT ç”Ÿæˆ | â€” |
| æ ·å¼ç³»ç»Ÿ | StyleProfile + presets | â€” |
| é¡¹ç›®æ–‡ä»¶ | .subgen ä¿å­˜/åŠ è½½ | â€” |
| äº‘ç«¯ Whisper | Groq / OpenAI API | groq, openai |
| OAuth ç™»å½• | Copilot / ChatGPT | httpx |
| ç»„ä»¶ç®¡ç†å™¨ | ä¸‹è½½/æ›´æ–°/æ£€æŸ¥ç»„ä»¶ | â€” |

### 2.2 å¯ä¸‹è½½ç»„ä»¶

| ç»„ä»¶ ID | æè¿° | æ¥æº | å¤§å° | å¹³å° |
|---------|------|------|------|------|
| `whisper-cpp-cuda` | whisper.cpp CUDA ç‰ˆ | SubGen Releases / è‡ªç¼–è¯‘ | ~15MB | Linux/Windows |
| `whisper-cpp-cpu` | whisper.cpp CPU ç‰ˆ | SubGen Releases / è‡ªç¼–è¯‘ | ~5MB | å…¨å¹³å° |
| `whisper-cpp-metal` | whisper.cpp Metal ç‰ˆ | SubGen Releases / è‡ªç¼–è¯‘ | ~8MB | macOS |
| `whisper-cpp-vulkan` | whisper.cpp Vulkan ç‰ˆ | SubGen Releases / è‡ªç¼–è¯‘ | ~10MB | å…¨å¹³å° |
| `model-whisper-tiny` | Whisper tiny æ¨¡å‹ | HuggingFace (ggerganov) | 75MB | å…¨å¹³å° |
| `model-whisper-base` | Whisper base æ¨¡å‹ | HuggingFace | 142MB | å…¨å¹³å° |
| `model-whisper-small` | Whisper small æ¨¡å‹ | HuggingFace | 466MB | å…¨å¹³å° |
| `model-whisper-medium` | Whisper medium æ¨¡å‹ | HuggingFace | 1.5GB | å…¨å¹³å° |
| `model-whisper-large-v3` | Whisper large-v3 æ¨¡å‹ | HuggingFace | 3.1GB | å…¨å¹³å° |
| `ffmpeg` | éŸ³è§†é¢‘å¤„ç† | FFmpeg å®˜æ–¹ / BtbN builds | ~80MB | å…¨å¹³å° |

### 2.3 æ¨¡å‹æ¨èç­–ç•¥

æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨æ¨èæ¨¡å‹å¤§å°ï¼š

| åœºæ™¯ | æ¨èæ¨¡å‹ | VRAM éœ€æ±‚ | åŸå›  |
|------|---------|----------|------|
| NVIDIA â‰¥8GB VRAM | large-v3 | ~3GB VRAM | æœ€ä½³è´¨é‡ |
| NVIDIA 4-8GB VRAM | medium | ~2GB VRAM | å¹³è¡¡ |
| NVIDIA <4GB VRAM | small | ~1GB VRAM | å¤Ÿç”¨ |
| Apple Silicon â‰¥16GB | large-v3 | å…±äº«å†…å­˜ | Metal åŠ é€Ÿ |
| Apple Silicon 8GB | medium | å…±äº«å†…å­˜ | å®‰å…¨é€‰æ‹© |
| CPU only | small æˆ– base | N/A | å¤ªæ…¢å°±æ¨èäº‘ç«¯ |

---

## 3. æ–°å¢æ¨¡å—ï¼š`src/components.py`

### 3.1 æ•°æ®ç»“æ„

```python
from dataclasses import dataclass, field
from typing import Optional, List
from pathlib import Path
from enum import Enum

class ComponentType(Enum):
    ENGINE = "engine"      # whisper-cpp äºŒè¿›åˆ¶
    MODEL = "model"        # GGML æ¨¡å‹æ–‡ä»¶
    TOOL = "tool"          # ffmpeg ç­‰å¤–éƒ¨å·¥å…·

class Platform(Enum):
    WINDOWS = "windows"
    MACOS_X64 = "macos-x64"
    MACOS_ARM64 = "macos-arm64"
    LINUX_X64 = "linux-x64"

@dataclass
class Component:
    """ç»„ä»¶å®šä¹‰"""
    id: str                          # "whisper-cpp-cuda"
    name: str                        # "whisper.cpp (CUDA)"
    type: ComponentType
    version: str                     # "1.7.3"
    description: str
    size_bytes: int                   # ä¸‹è½½å¤§å°
    urls: dict[str, str]             # platform -> download URL
    sha256: dict[str, str]           # platform -> checksum
    requires: List[str] = field(default_factory=list)  # ä¾èµ–çš„å…¶ä»–ç»„ä»¶
    install_path: str = ""           # ç›¸å¯¹äº ~/.subgen/ çš„å®‰è£…è·¯å¾„
    executable: str = ""             # å¯æ‰§è¡Œæ–‡ä»¶å

@dataclass
class InstalledComponent:
    """å·²å®‰è£…çš„ç»„ä»¶"""
    id: str
    version: str
    path: Path
    installed_at: str                # ISO timestamp
    size_bytes: int
```

### 3.2 ç»„ä»¶æ³¨å†Œè¡¨

ç»„ä»¶å…ƒæ•°æ®ä¸ç¡¬ç¼–ç åœ¨ exe é‡Œï¼Œè€Œæ˜¯ä»è¿œç¨‹è·å–ï¼ˆæ”¯æŒç‰ˆæœ¬æ›´æ–°ï¼‰ï¼š

```
è·å–é¡ºåºï¼š
1. æœ¬åœ°ç¼“å­˜: ~/.subgen/components.json (24h æœ‰æ•ˆ)
2. è¿œç¨‹: https://github.com/lgezyxr/subgen/releases/latest/download/components.json
3. å†…ç½® fallback: exe æ‰“åŒ…æ—¶çš„ components.json å¿«ç…§
```

**components.json ç¤ºä¾‹ï¼š**

```json
{
  "version": "1",
  "updated": "2026-02-26T00:00:00Z",
  "components": {
    "whisper-cpp-cuda": {
      "name": "whisper.cpp (CUDA)",
      "type": "engine",
      "version": "1.7.3",
      "description": "Local speech recognition with NVIDIA GPU acceleration",
      "urls": {
        "windows": "https://github.com/lgezyxr/subgen/releases/download/components-v1/whisper-cpp-cuda-windows-x64.zip",
        "linux-x64": "https://github.com/lgezyxr/subgen/releases/download/components-v1/whisper-cpp-cuda-linux-x64.tar.gz"
      },
      "sha256": {
        "windows": "abc123...",
        "linux-x64": "def456..."
      },
      "size_bytes": 15728640,
      "install_path": "bin/whisper-cpp",
      "executable": "whisper-cpp"
    },
    "model-whisper-large-v3": {
      "name": "Whisper Large V3",
      "type": "model",
      "version": "1.0",
      "description": "Best quality, 3.1GB, requires â‰¥8GB VRAM",
      "urls": {
        "*": "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3.bin"
      },
      "sha256": {
        "*": "..."
      },
      "size_bytes": 3326234624,
      "install_path": "models/whisper/ggml-large-v3.bin"
    },
    "ffmpeg": {
      "name": "FFmpeg",
      "type": "tool",
      "version": "7.1",
      "description": "Audio/video processing (required for video input)",
      "urls": {
        "windows": "https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-gpl.zip",
        "linux-x64": "https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz",
        "macos-arm64": "https://evermeet.cx/ffmpeg/getrelease/ffmpeg/zip",
        "macos-x64": "https://evermeet.cx/ffmpeg/getrelease/ffmpeg/zip"
      },
      "size_bytes": 83886080,
      "install_path": "bin/ffmpeg",
      "executable": "ffmpeg"
    }
  }
}
```

### 3.3 ComponentManager æ ¸å¿ƒç±»

```python
class ComponentManager:
    """ç®¡ç†ç»„ä»¶çš„ä¸‹è½½ã€å®‰è£…ã€æ›´æ–°ã€åˆ é™¤"""

    def __init__(self, base_dir: Path = None):
        """
        base_dir: ~/.subgen/
        çŠ¶æ€æ–‡ä»¶: ~/.subgen/installed.json
        """

    # === æŸ¥è¯¢ ===
    def list_available(self) -> list[Component]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç»„ä»¶ï¼ˆä»æ³¨å†Œè¡¨è·å–ï¼‰"""

    def list_installed(self) -> list[InstalledComponent]:
        """åˆ—å‡ºå·²å®‰è£…çš„ç»„ä»¶"""

    def is_installed(self, component_id: str) -> bool:
        """æ£€æŸ¥ç»„ä»¶æ˜¯å¦å·²å®‰è£…"""

    def get_path(self, component_id: str) -> Optional[Path]:
        """è·å–å·²å®‰è£…ç»„ä»¶çš„è·¯å¾„"""

    def needs_update(self, component_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ›´æ–°"""

    # === å®‰è£… ===
    def install(self, component_id: str,
                on_progress: Callable[[int, int], None] = None) -> Path:
        """
        ä¸‹è½½å¹¶å®‰è£…ç»„ä»¶ã€‚
        - æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        - SHA256 æ ¡éªŒ
        - è‡ªåŠ¨è§£å‹ zip/tar.gz
        - å†™å…¥ installed.json
        - è¿”å›å®‰è£…è·¯å¾„
        """

    def install_model(self, model_name: str,
                      on_progress: Callable[[int, int], None] = None) -> Path:
        """å®‰è£… Whisper æ¨¡å‹ï¼ˆç®€åŒ–æ¥å£ï¼‰
        model_name: tiny | base | small | medium | large-v3
        """

    # === ç®¡ç† ===
    def uninstall(self, component_id: str) -> bool:
        """åˆ é™¤å·²å®‰è£…çš„ç»„ä»¶"""

    def update(self, component_id: str) -> bool:
        """æ›´æ–°ç»„ä»¶åˆ°æœ€æ–°ç‰ˆæœ¬"""

    def disk_usage(self) -> dict[str, int]:
        """å„ç»„ä»¶å ç”¨ç©ºé—´"""

    # === å·¥å…· ===
    def find_ffmpeg(self) -> Optional[Path]:
        """æŸ¥æ‰¾ ffmpeg: 1) ~/.subgen/bin/ 2) PATH 3) None"""

    def find_whisper_engine(self) -> Optional[Path]:
        """æŸ¥æ‰¾ whisper-cpp å¼•æ“"""

    def find_whisper_model(self, model_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å·²ä¸‹è½½çš„ Whisper æ¨¡å‹"""

    # === å†…éƒ¨ ===
    def _download(self, url: str, dest: Path,
                  on_progress: Callable[[int, int], None] = None,
                  sha256: str = None) -> Path:
        """ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒè¿›åº¦å›è°ƒ + æ–­ç‚¹ç»­ä¼  + SHA256 æ ¡éªŒ"""

    def _detect_platform(self) -> str:
        """æ£€æµ‹å½“å‰å¹³å°: windows | linux-x64 | macos-x64 | macos-arm64"""

    def _refresh_registry(self) -> dict:
        """åˆ·æ–°ç»„ä»¶æ³¨å†Œè¡¨ï¼ˆè¿œç¨‹ â†’ ç¼“å­˜ï¼‰"""
```

### 3.4 çŠ¶æ€æ–‡ä»¶ `~/.subgen/installed.json`

```json
{
  "components": {
    "whisper-cpp-cuda": {
      "version": "1.7.3",
      "path": "/home/user/.subgen/bin/whisper-cpp/whisper-cpp",
      "installed_at": "2026-02-26T12:00:00Z",
      "size_bytes": 15728640
    },
    "model-whisper-large-v3": {
      "version": "1.0",
      "path": "/home/user/.subgen/models/whisper/ggml-large-v3.bin",
      "installed_at": "2026-02-26T12:05:00Z",
      "size_bytes": 3326234624
    }
  },
  "registry_cached_at": "2026-02-26T12:00:00Z"
}
```

---

## 4. whisper.cpp åç«¯é›†æˆ

### 4.1 æ–°å¢ `src/transcribe_cpp.py`

whisper.cpp çš„ Python ç»‘å®šæœ‰ä¸¤ä¸ªä¸»è¦é€‰æ‹©ï¼š

| åº“ | ç‰¹ç‚¹ | æ‰“åŒ…å‹å¥½åº¦ |
|----|------|-----------|
| `pywhispercpp` | Pythonic APIï¼Œè‡ªå¸¦é¢„ç¼–è¯‘ wheel | âŒ ç¼–è¯‘æ—¶ç»‘å®š |
| ç›´æ¥è°ƒç”¨äºŒè¿›åˆ¶ | é›¶ä¾èµ–ï¼Œé€šè¿‡ subprocess è°ƒç”¨ whisper-cpp CLI | âœ… å®Œç¾ |

**é€‰æ‹©ï¼šç›´æ¥è°ƒç”¨ whisper-cpp äºŒè¿›åˆ¶ï¼ˆsubprocessï¼‰**

åŸå› ï¼š
- ä¸éœ€è¦åœ¨ exe é‡Œæ‰“åŒ… C++ ç¼–è¯‘äº§ç‰©
- whisper-cpp CLI è¾“å‡º JSON/SRT/VTT æ ¼å¼ï¼Œå®¹æ˜“è§£æ
- äºŒè¿›åˆ¶å¯ç‹¬ç«‹æ›´æ–°ï¼Œä¸éœ€è¦é‡æ–°æ‰“åŒ… exe
- é¿å… CUDA/Metal ç¼–è¯‘é—®é¢˜

```python
# src/transcribe_cpp.py

"""whisper.cpp backend â€” calls whisper-cpp binary via subprocess"""

import json
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable

from .transcribe import Segment, Word
from .components import ComponentManager
from .logger import debug


def transcribe_cpp(
    audio_path: Path,
    config: Dict[str, Any],
    on_progress: Optional[Callable[[int, int], None]] = None
) -> List[Segment]:
    """
    ä½¿ç”¨ whisper.cpp äºŒè¿›åˆ¶è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚

    æµç¨‹ï¼š
    1. é€šè¿‡ ComponentManager æ‰¾åˆ° whisper-cpp å¼•æ“å’Œæ¨¡å‹
    2. å¦‚æœæ²¡æœ‰åˆ™æç¤ºå®‰è£…
    3. è°ƒç”¨ whisper-cpp CLIï¼Œè¾“å‡º JSON
    4. è§£æ JSON ä¸º Segment åˆ—è¡¨
    """
    cm = ComponentManager()

    # æ‰¾å¼•æ“
    engine_path = cm.find_whisper_engine()
    if not engine_path:
        raise RuntimeError(
            "whisper.cpp engine not found.\n"
            "Run: subgen install whisper-cpp\n"
            "Or use cloud Whisper: subgen init"
        )

    # æ‰¾æ¨¡å‹
    model_name = config['whisper'].get('local_model', 'large-v3')
    model_path = cm.find_whisper_model(model_name)
    if not model_path:
        raise RuntimeError(
            f"Whisper model '{model_name}' not found.\n"
            f"Run: subgen install model-whisper-{model_name}"
        )

    # æ„å»ºå‘½ä»¤
    cmd = [
        str(engine_path),
        "-m", str(model_path),
        "-f", str(audio_path),
        "--output-json",           # JSON è¾“å‡º
        "--print-progress",        # æ‰“å°è¿›åº¦åˆ° stderr
        "-t", str(config['whisper'].get('threads', 4)),
    ]

    # è¯­è¨€
    source_lang = config['whisper'].get('source_language')
    if source_lang:
        cmd.extend(["-l", source_lang])

    debug("transcribe_cpp: running %s", " ".join(cmd))

    # æ‰§è¡Œ
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )

    # è¯»å– stderr è·å–è¿›åº¦
    stderr_lines = []
    for line in process.stderr:
        stderr_lines.append(line)
        # whisper.cpp è¾“å‡ºç±»ä¼¼: "whisper_print_progress_callback: progress = 42%"
        if "progress =" in line and on_progress:
            try:
                pct = int(line.split("=")[1].strip().rstrip("%"))
                on_progress(pct, 100)
            except (ValueError, IndexError):
                pass

    stdout = process.stdout.read()
    process.wait()

    if process.returncode != 0:
        raise RuntimeError(
            f"whisper.cpp failed (exit {process.returncode}):\n"
            + "".join(stderr_lines[-10:])
        )

    # è§£æ JSON è¾“å‡º
    return _parse_whisper_json(stdout)


def _parse_whisper_json(json_str: str) -> List[Segment]:
    """è§£æ whisper.cpp çš„ JSON è¾“å‡ºä¸º Segment åˆ—è¡¨"""
    data = json.loads(json_str)

    segments = []
    for item in data.get("transcription", []):
        # æ—¶é—´æˆ³æ ¼å¼: "00:00:01.234" â†’ ç§’
        start = _timestamp_to_seconds(item["timestamps"]["from"])
        end = _timestamp_to_seconds(item["timestamps"]["to"])
        text = item["text"].strip()

        if not text:
            continue

        # word-level timestamps (å¦‚æœæœ‰)
        words = []
        for token in item.get("tokens", []):
            if "timestamps" in token and token.get("text", "").strip():
                w_start = _timestamp_to_seconds(token["timestamps"]["from"])
                w_end = _timestamp_to_seconds(token["timestamps"]["to"])
                words.append(Word(
                    text=token["text"].strip(),
                    start=w_start,
                    end=w_end
                ))

        seg = Segment(
            start=start,
            end=end,
            text=text,
            words=words,
            no_speech_prob=item.get("no_speech_prob", 0.0)
        )
        segments.append(seg)

    return segments


def _timestamp_to_seconds(ts: str) -> float:
    """è½¬æ¢ whisper.cpp æ—¶é—´æˆ³ "HH:MM:SS.mmm" â†’ ç§’"""
    parts = ts.split(":")
    h = int(parts[0])
    m = int(parts[1])
    s = float(parts[2])
    return h * 3600 + m * 60 + s
```

### 4.2 ä¿®æ”¹ `src/transcribe.py`

åœ¨ provider åˆ†æ”¯ä¸­åŠ å…¥ `cpp`ï¼š

```python
# åœ¨ transcribe() å‡½æ•°çš„ provider åˆ†æ”¯ä¸­æ·»åŠ ï¼š

elif provider == 'cpp':
    from .transcribe_cpp import transcribe_cpp
    segments = transcribe_cpp(audio_path, config)
```

### 4.3 å‘åå…¼å®¹

| provider | åç«¯ | ä¾èµ– | çŠ¶æ€ |
|----------|------|------|------|
| `local` | faster-whisper (PyTorch) | pip install | ä¿ç•™ï¼Œå¼€å‘è€…ç”¨ |
| `cpp` | whisper.cpp (subprocess) | subgen install | **æ–°å¢**ï¼Œexe ç”¨æˆ·ç”¨ |
| `mlx` | mlx-whisper | pip install | ä¿ç•™ï¼ŒMac å¼€å‘è€…ç”¨ |
| `groq` | Groq API | å†…ç½® | ä¿ç•™ |
| `openai` | OpenAI API | å†…ç½® | ä¿ç•™ |

exe ç‰ˆ wizard ä¸­ä¸æ˜¾ç¤º `local`ï¼ˆéœ€è¦ PyTorchï¼‰å’Œ `mlx`ï¼ˆéœ€è¦ pipï¼‰ï¼Œåªæ˜¾ç¤º `cpp`ã€`groq`ã€`openai`ã€‚

---

## 5. CLI æ–°å‘½ä»¤

### 5.1 `subgen install` â€” å®‰è£…ç»„ä»¶

```bash
# å®‰è£…æœ¬åœ° Whisper å¼•æ“ï¼ˆè‡ªåŠ¨é€‰ CUDA/Metal/CPUï¼‰
subgen install whisper

# å®‰è£…æŒ‡å®šæ¨¡å‹
subgen install model large-v3

# å®‰è£… FFmpeg
subgen install ffmpeg

# ä¸€æ­¥åˆ°ä½ï¼ˆå¼•æ“ + æ¨èæ¨¡å‹ï¼‰
subgen install whisper --with-model
```

äº¤äº’ç¤ºä¾‹ï¼š

```
$ subgen install whisper

ğŸ” Detecting hardware...
  âœ“ NVIDIA GeForce RTX 3060 (12GB VRAM)
  âœ“ CUDA 12.4

ğŸ“¥ Installing whisper.cpp (CUDA)...
  Downloading: 15.2 MB [==================] 100% (2.1s)
  Verifying checksum... âœ“
  Installed to: ~/.subgen/bin/whisper-cpp

ğŸ’¡ No Whisper model found. Install one?
  1. tiny     (75 MB)   â€” Fast, lower quality
  2. base     (142 MB)  â€” Balanced for quick tasks
  3. small    (466 MB)  â€” Good quality
  4. medium   (1.5 GB)  â€” Great quality
  5. large-v3 (3.1 GB)  â€” Best quality â­ (recommended for 12GB VRAM)

Select model [5]: 5

ğŸ“¥ Downloading Whisper large-v3 model...
  Downloading: 3.1 GB [=========>         ] 31% (ETA: 45s)
```

### 5.2 `subgen doctor` â€” è¯Šæ–­ç¯å¢ƒ

```bash
$ subgen doctor

ğŸ¥ SubGen Environment Check
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Config:     ~/.subgen/config.yaml âœ“
  FFmpeg:     /usr/bin/ffmpeg (7.0.1) âœ“
  Whisper:    whisper.cpp CUDA (~/.subgen/bin/) âœ“
  Model:      large-v3 (3.1 GB) âœ“
  LLM:        Copilot (authenticated) âœ“
  GPU:        NVIDIA RTX 3060 (12GB) âœ“
  Disk:       ~/.subgen/ using 3.4 GB

  Status: âœ… Ready to go!
```

```bash
$ subgen doctor

ğŸ¥ SubGen Environment Check
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Config:     Not found âœ— (run: subgen init)
  FFmpeg:     Not found âœ— (run: subgen install ffmpeg)
  Whisper:    Not configured âœ—
  LLM:        Not configured âœ—
  GPU:        No NVIDIA GPU detected
  Disk:       ~/.subgen/ not created

  Status: âŒ Run 'subgen init' to get started
```

### 5.3 `subgen uninstall` â€” åˆ é™¤ç»„ä»¶

```bash
subgen uninstall model large-v3    # é‡Šæ”¾ 3.1GB
subgen uninstall whisper           # åˆ é™¤å¼•æ“
subgen uninstall ffmpeg
```

### 5.4 `subgen update` â€” æ›´æ–°ç»„ä»¶

```bash
subgen update               # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ›´æ–°
subgen update whisper        # æ›´æ–° whisper.cpp å¼•æ“
```

---

## 6. Setup Wizard æ”¹é€ 

### 6.1 æ ¸å¿ƒåŸåˆ™ï¼šinit = ä¸€ç«™å¼è®¾ç½®

**ç”¨æˆ·è·‘å®Œ `subgen init` åå°±èƒ½ç›´æ¥ `subgen run`ã€‚** æ‰€æœ‰éœ€è¦çš„ç»„ä»¶ï¼ˆå¼•æ“ã€æ¨¡å‹ã€FFmpegã€OAuthï¼‰éƒ½åœ¨ init è¿‡ç¨‹ä¸­å®Œæˆï¼Œä¸éœ€è¦ç”¨æˆ·å†æ‰‹åŠ¨è·‘ install å‘½ä»¤ã€‚

install/doctor/update/uninstall ä½œä¸ºé«˜çº§ç®¡ç†å‘½ä»¤ä¿ç•™ï¼Œä½†æ™®é€šç”¨æˆ·ä¸éœ€è¦ç¢°ã€‚

### 6.2 å®Œæ•´ init æµç¨‹

```
$ subgen init

ğŸ¬ SubGen Setup Wizard
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Detecting hardware...
  âœ“ NVIDIA GeForce RTX 3060 (12GB VRAM)
  âœ“ CUDA 12.4

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“¢ Step 1/4: Speech Recognition

  How do you want to transcribe audio?

  1. â˜ï¸  Groq (Cloud)        â€” Free, fast, no GPU needed â­
  2. ğŸ’» Local (whisper.cpp)  â€” Free, offline, needs download (~3.1GB)
  3. â˜ï¸  OpenAI Whisper API  â€” $0.006/min, most reliable

> 2

ğŸ“¥ Setting up local speech recognition...

  Downloading whisper.cpp engine (CUDA)...
  15.2 MB [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% âœ“

  Recommended model for your GPU (12GB): large-v3 (best quality)
  Other options: tiny (75MB) | base (142MB) | small (466MB) | medium (1.5GB)
  
  Download large-v3? [Y/n]: y
  
  Downloading Whisper large-v3 model...
  3.1 GB [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% (1m 23s) âœ“

  âœ… Local Whisper ready

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸŒ Step 2/4: Translation

  1. ğŸ™ GitHub Copilot      â€” Use your Copilot subscription
  2. ğŸ’¬ ChatGPT Plus/Pro    â€” Use your ChatGPT subscription
  3. ğŸ”‘ OpenAI API          â€” Pay per use ($0.15/M tokens)
  4. ğŸ‡¨ğŸ‡³ DeepSeek           â€” Cheap, good for Chinese
  5. ğŸ  Ollama (Local)      â€” Free, requires local setup

> 1

  Starting GitHub OAuth login...
  
  ğŸ‘‰ Open this URL: https://github.com/login/device
  ğŸ‘‰ Enter code: ABCD-1234
  
  Waiting for authorization... âœ“
  âœ… GitHub Copilot connected

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Step 3/4: FFmpeg

  ğŸ” Checking FFmpeg...
  âœ— FFmpeg not found

  FFmpeg is required for video processing.

  ğŸ“¥ Download FFmpeg automatically? [Y/n]: y

  Downloading FFmpeg...
  80 MB [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% âœ“
  âœ… FFmpeg installed to ~/.subgen/bin/

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Step 4/4: Defaults

  Target language [zh]: zh
  Enable bilingual subtitles? [y/N]: y
  Default subtitle format (srt/ass/vtt) [srt]: ass
  Style preset (default/netflix/fansub/minimal) [default]: fansub

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… All set! SubGen is ready to use.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Summary:
  â”€â”€â”€â”€â”€â”€â”€â”€
  Whisper:   Local (whisper.cpp CUDA + large-v3)
  LLM:       GitHub Copilot (claude-sonnet-4)
  FFmpeg:    ~/.subgen/bin/ffmpeg
  Language:  zh (ä¸­æ–‡)
  Bilingual: yes
  Format:    ASS (fansub preset)
  Disk used: 3.3 GB (~/.subgen/)

  Config: ~/.subgen/config.yaml

  ğŸš€ Try it now:
    subgen run movie.mp4
```

### 6.3 init å†…éƒ¨é€»è¾‘

```python
def run_setup_wizard():
    """ä¸€ç«™å¼è®¾ç½®ï¼Œå®Œæˆåç”¨æˆ·å¯ä»¥ç›´æ¥ subgen run"""

    cm = ComponentManager()
    hw = detect_hardware()

    # Step 1: Whisper
    whisper_provider = prompt_whisper_choice(hw)

    if whisper_provider == 'cpp':
        # è‡ªåŠ¨ä¸‹è½½å¼•æ“ï¼ˆæ ¹æ®ç¡¬ä»¶é€‰ CUDA/Metal/CPUï¼‰
        engine_variant = pick_engine_variant(hw)
        cm.install(f"whisper-cpp-{engine_variant}", on_progress=rich_progress)

        # è‡ªåŠ¨æ¨è + ä¸‹è½½æ¨¡å‹
        recommended_model = recommend_model(hw)
        model_choice = prompt_model_choice(recommended_model)
        cm.install(f"model-whisper-{model_choice}", on_progress=rich_progress)

    elif whisper_provider == 'groq':
        groq_key = prompt_api_key("Groq", "https://console.groq.com/keys")

    # Step 2: LLM
    llm_provider = prompt_llm_choice()
    if llm_provider in ('copilot', 'chatgpt'):
        run_oauth(llm_provider)
    elif needs_key(llm_provider):
        api_key = prompt_api_key(llm_provider)

    # Step 3: FFmpegï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œæ²¡æœ‰å°±ä¸‹è½½ï¼‰
    ffmpeg = cm.find_ffmpeg() or shutil.which('ffmpeg')
    if not ffmpeg:
        if confirm("Download FFmpeg automatically?"):
            cm.install("ffmpeg", on_progress=rich_progress)
        else:
            warn("FFmpeg not installed. Video processing won't work.")

    # Step 4: é»˜è®¤è¾“å‡ºè®¾ç½®
    target_lang = prompt("Target language", default="zh")
    bilingual = confirm("Enable bilingual subtitles?", default=False)
    format = prompt_choice("Subtitle format", ["srt", "ass", "vtt"], default="srt")
    if format == "ass":
        preset = prompt_choice("Style preset", ["default", "netflix", "fansub", "minimal"])

    # ä¿å­˜ config
    save_config(...)
    print_summary(...)
```

### 6.4 é‡æ–°è¿è¡Œ init

ç”¨æˆ·å¯ä»¥éšæ—¶ `subgen init` é‡æ–°è®¾ç½®ã€‚å¦‚æœå·²æœ‰é…ç½®ï¼Œæ˜¾ç¤ºå½“å‰è®¾ç½®å¹¶å…è®¸ä¿®æ”¹ï¼š

```
$ subgen init

ğŸ¬ SubGen Setup Wizard
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â„¹ï¸ Existing config found. Current settings:
  
    Whisper: Local (whisper.cpp CUDA + large-v3) âœ“
    LLM:     GitHub Copilot âœ“
    FFmpeg:  ~/.subgen/bin/ffmpeg âœ“

  Reconfigure? [y/N]: y
  
  (è¿›å…¥æ­£å¸¸ wizard æµç¨‹...)
```

### 6.5 `subgen run` é›¶é…ç½®æ£€æµ‹

å¦‚æœç”¨æˆ·ç›´æ¥è·‘ `subgen run` ä½†æ²¡æœ‰ configï¼Œè‡ªåŠ¨è§¦å‘ initï¼š

```python
# subgen.py run å‘½ä»¤å…¥å£
def run(input_path, ...):
    config = load_config()
    if config is None:
        print("âš ï¸  No config found. Let's set up SubGen first.\n")
        run_setup_wizard()
        config = load_config()
    # ç»§ç»­æ­£å¸¸æµç¨‹...
```

---

## 7. é…ç½®æ–‡ä»¶å˜æ›´

### 7.1 `config.yaml` æ–°å¢å­—æ®µ

```yaml
whisper:
  # provider: groq | openai | cpp | local | mlx
  provider: "cpp"

  # whisper.cpp ä¸“ç”¨é…ç½®
  cpp_model: "large-v3"     # æ¨¡å‹åç§°ï¼ˆéœ€å·²ä¸‹è½½ï¼‰
  cpp_threads: 4            # CPU çº¿ç¨‹æ•°
  cpp_gpu_layers: 0         # GPU åŠ é€Ÿå±‚æ•° (0 = è‡ªåŠ¨)

  # ä¿ç•™åŸæœ‰é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
  local_model: "large-v3"   # for provider: local (faster-whisper)
  device: "cuda"
```

### 7.2 è·¯å¾„æœç´¢ä¼˜å…ˆçº§

exe æ¨¡å¼ä¸‹ï¼Œconfig.yaml å’Œå…¶ä»–æ–‡ä»¶çš„æœç´¢é¡ºåºï¼š

```
config.yaml:
  1. --config å‘½ä»¤è¡Œå‚æ•°
  2. å½“å‰ç›®å½• ./config.yaml
  3. ~/.subgen/config.yaml

ffmpeg:
  1. ~/.subgen/bin/ffmpeg
  2. PATH ä¸­çš„ ffmpeg

whisper-cpp:
  1. ~/.subgen/bin/whisper-cpp/whisper-cpp
  2. PATH ä¸­çš„ whisper-cpp

æ¨¡å‹:
  1. ~/.subgen/models/whisper/ggml-{name}.bin
  2. config ä¸­æŒ‡å®šçš„ç»å¯¹è·¯å¾„
```

---

## 8. whisper.cpp äºŒè¿›åˆ¶åˆ†å‘ç­–ç•¥

### 8.1 æ–¹æ¡ˆï¼šè‡ªå»º GitHub Release

åœ¨ subgen ä»“åº“åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ release tagï¼ˆå¦‚ `components-v1`ï¼‰ï¼Œä¸Šä¼ é¢„ç¼–è¯‘çš„ whisper.cpp äºŒè¿›åˆ¶ï¼š

```
Release: components-v1
  â”œâ”€â”€ whisper-cpp-cuda-windows-x64.zip
  â”œâ”€â”€ whisper-cpp-cuda-linux-x64.tar.gz
  â”œâ”€â”€ whisper-cpp-cpu-windows-x64.zip
  â”œâ”€â”€ whisper-cpp-cpu-linux-x64.tar.gz
  â”œâ”€â”€ whisper-cpp-cpu-macos-x64.tar.gz
  â”œâ”€â”€ whisper-cpp-metal-macos-arm64.tar.gz
  â”œâ”€â”€ components.json              â† ç»„ä»¶æ³¨å†Œè¡¨
  â””â”€â”€ checksums.sha256
```

### 8.2 ç¼–è¯‘ CI

ç”¨ GitHub Actions ç¼–è¯‘ whisper.cppï¼ˆä» ggml-org/whisper.cpp æºç ï¼‰ï¼š

```yaml
# .github/workflows/build-components.yml
# æ‰‹åŠ¨è§¦å‘ï¼Œç¼–è¯‘å„å¹³å°çš„ whisper-cpp äºŒè¿›åˆ¶
# ä¸Šä¼ åˆ° components-v* release
```

### 8.3 æ¨¡å‹ä¸‹è½½

æ¨¡å‹ç›´æ¥ä» HuggingFace ä¸‹è½½ï¼ˆå®˜æ–¹ä»“åº“ï¼‰ï¼Œä¸éœ€è¦æˆ‘ä»¬æ‰˜ç®¡ï¼š

```
https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-{model}.bin
```

---

## 9. å®æ–½è®¡åˆ’

### Phase 1ï¼šç»„ä»¶ç®¡ç†å™¨ï¼ˆ2-3hï¼‰
- [ ] `src/components.py` â€” ComponentManager æ ¸å¿ƒç±»
- [ ] `components.json` â€” ç»„ä»¶æ³¨å†Œè¡¨ï¼ˆå…ˆå†…ç½®ï¼Œåè¿œç¨‹ï¼‰
- [ ] `~/.subgen/` ç›®å½•ç»“æ„åˆå§‹åŒ–
- [ ] ä¸‹è½½é€»è¾‘ï¼šè¿›åº¦æ¡ + SHA256 æ ¡éªŒ + æ–­ç‚¹ç»­ä¼ 
- [ ] `installed.json` çŠ¶æ€ç®¡ç†
- [ ] æµ‹è¯•

### Phase 2ï¼šwhisper.cpp åç«¯ï¼ˆ1-2hï¼‰
- [ ] `src/transcribe_cpp.py` â€” subprocess è°ƒç”¨ whisper-cpp
- [ ] ä¿®æ”¹ `src/transcribe.py` åŠ å…¥ `cpp` provider
- [ ] JSON è¾“å‡ºè§£æ â†’ Segment åˆ—è¡¨
- [ ] è¿›åº¦å›è°ƒï¼ˆè§£æ stderr è¿›åº¦ï¼‰
- [ ] æµ‹è¯•

### Phase 3ï¼šCLI å‘½ä»¤ï¼ˆ1-2hï¼‰
- [ ] `subgen install` â€” å®‰è£…ç»„ä»¶
- [ ] `subgen uninstall` â€” åˆ é™¤ç»„ä»¶
- [ ] `subgen doctor` â€” ç¯å¢ƒè¯Šæ–­
- [ ] `subgen update` â€” æ›´æ–°ç»„ä»¶
- [ ] ä¿®æ”¹ wizard é€‚é… exe æ¨¡å¼

### Phase 4ï¼šæ‰“åŒ… + CIï¼ˆ2-3hï¼‰
- [ ] `subgen.spec` â€” PyInstaller é…ç½®
- [ ] `.github/workflows/release.yml` â€” è‡ªåŠ¨æ‰“åŒ… 4 å¹³å° exe
- [ ] `.github/workflows/build-components.yml` â€” ç¼–è¯‘ whisper-cpp äºŒè¿›åˆ¶
- [ ] æµ‹è¯•å‘å¸ƒæµç¨‹
- [ ] æ›´æ–° README åŠ ä¸‹è½½è¯´æ˜

### Phase 5ï¼šéªŒè¯ï¼ˆ1hï¼‰
- [ ] Windows exe æµ‹è¯•
- [ ] macOS arm64 æµ‹è¯•
- [ ] Linux x64 æµ‹è¯•
- [ ] å…¨æµç¨‹ï¼šä¸‹è½½ exe â†’ init â†’ install whisper â†’ run video.mp4

**æ€»è®¡çº¦ 8-12h å·¥ä½œé‡ã€‚**

---

## 10. æ³¨æ„äº‹é¡¹

- **æ€æ¯’è½¯ä»¶**ï¼šPyInstaller æ‰“åŒ…çš„ exe å¯èƒ½è¢«è¯¯æŠ¥ï¼Œéœ€è¦ç­¾åæˆ–ä¸Šä¼  VirusTotal ç™½åå•
- **macOS Gatekeeper**ï¼šéœ€è¦ codesign æˆ–å‘ŠçŸ¥ç”¨æˆ· `xattr -d com.apple.quarantine subgen`
- **CUDA ç‰ˆæœ¬å…¼å®¹**ï¼šwhisper-cpp CUDA ç‰ˆç¼–è¯‘æ—¶ç»‘å®šç‰¹å®š CUDA ç‰ˆæœ¬ï¼Œéœ€è¦æä¾›å¤šç‰ˆæœ¬æˆ–ç”¨ CUDA runtime åŠ¨æ€é“¾æ¥
- **æ¨¡å‹è®¸å¯**ï¼šWhisper æ¨¡å‹æ˜¯ MIT è®¸å¯ï¼Œå¯ä»¥è‡ªç”±åˆ†å‘
- **æ–­ç‚¹ç»­ä¼ **ï¼šå¤§æ¨¡å‹æ–‡ä»¶ï¼ˆ3.1GBï¼‰ä¸‹è½½ä¸­æ–­å¾ˆå¸¸è§ï¼Œå¿…é¡»æ”¯æŒ HTTP Range ç»­ä¼ 
